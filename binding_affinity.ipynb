{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9AaWlZE6BYE6C/Z1NEyuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/istender15/bme5900-p1/blob/main/binding_affinity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "PBogGCspcOJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kdYJtQYzRWda"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from transformers import AutoTokenizer, EsmForMaskedLM\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
        "# This is used to convert protein sequences into a format suitable for the model.\n",
        "model = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
        "# This is the ESM-2 model, specifically built for protein sequences.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs4kyUdtRmwc",
        "outputId": "4f5eafe3-dbdb-494e-abff-bc97e968e138"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# this will set the code to use GPUs if available\n",
        "model = model.to(device)\n",
        "# casting the model to the GPU\n"
      ],
      "metadata": {
        "id": "sJBdiIUFRoix"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Binder Affinity"
      ],
      "metadata": {
        "id": "4nz0xW3scc2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(base_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the protein of interest and its potential binders\n",
        "protein_of_interest = \"EKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYDLSFLKTIQEVAGYVLIALNTVERIPLENLQIIRGNMYYENSYALAVLSNYDANKTGLKELPMRNLQEILHGAVRFSNNPALCNVESIQWRDIVSSDFLSNMSMDFQNHLGSCQKCDPSCPNGSCWGAGEENCQKLTKIICAQQCSGRCRGKSPSDCCHNQCAAGCTGPRESDCLVCRKFRDEATCKDTCPPLMLYNPTTYQMDVNPEGKYSFGATCVKKCPRNYVVTDHGSCVRACGADSYEMEEDGVRKCKKCEGPCRKVCNGIGIGEFKDSLSINATNIKHFKNCTSISGDLHILPVAFRGDSFTHTPPLDPQELDILKTVKEITGFLLIQAWPENRTDLHAFENLEIIRGRTKQHGQFSLAVVSLNITSLGLRSLKEISDGDVIISGNKNLCYANTINWKKLFGTSGQKTKIISNRGENSCKATGQVCHALCSPEGCWGPEPRDCVSCRNVSRGRECVDKCNLLEGEPREFVENSECIQCHPECLPQAMNITCTGRGPDNCIQCAHYIDGPHCVKT\"\n",
        "potential_binders = [\n",
        "    \"KPQRKTYYGNMKGREDYEPEQSKEVYAKKFASKTEEELEEVIKEEKAEIEKKKKQLEEDIKAGKVTEYNPKVKVITPVYPSEYKEVEE\", #my protein\n",
        "    \"SEEEEERERALKEIIEETRRELKAAKAKHGKVVVVLIMASSTLEPEFILELSKALIKEMKSLFPNVVLIIVVVGLAPASLLARIRDVSLELAKYAKSLGIKVIVIVGNENEAVFVPAFEALGVEVIVDRTIIEIAAEELGLSEEEVLARFAAAAELLDELFAADPSLRERYARLDVAGATELLLERLRELFGAKVERHERLITVEVERVLTPDERRRVTAILLTPEAAREVVERLVDLVVDLILEKIAEGHNVLVLVFTPTIALAREVAALFEERRPLLEEAGAAVIIRLVARDPDTFLI\", #ben protein\n",
        "    \"LEEKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYDLSFLKTIQEVAGYVLIALNTVERIPLENLQIIRGNMYYENSYALAVLSNYDANKTGLKELPMRNLQEILHGAVRFSNNPALCNVESIQWRDIVSSDFLSNMSMDFQNHLGSCQKCDPSCPNGSCW\", #noah protein\n",
        "    \"GRPPKNVKVSGVDGNSATISFDLADENDKYILIMIGPANDPNSWTSWWLPHETSYLSISNLPPGAEYQVTFMMVRPGKMSPPITQDFKC\" # jackson swalley protein\n",
        "]  # Add potential binding sequences here\n",
        "\n",
        "def compute_mlm_loss(protein, binder, iterations=3):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Concatenate protein sequences with a separator\n",
        "        concatenated_sequence = protein + \":\" + binder\n",
        "\n",
        "        # Mask a subset of amino acids in the concatenated sequence (excluding the separator)\n",
        "        tokens = list(concatenated_sequence)\n",
        "        mask_rate = 0.15  # For instance, masking 15% of the sequence\n",
        "        num_mask = int(len(tokens) * mask_rate)\n",
        "\n",
        "        # Exclude the separator from potential mask indices\n",
        "        available_indices = [i for i, token in enumerate(tokens) if token != \":\"]\n",
        "        probs = torch.ones(len(available_indices))\n",
        "        mask_indices = torch.multinomial(probs, num_mask, replacement=False)\n",
        "\n",
        "        for idx in mask_indices:\n",
        "            tokens[available_indices[idx]] = tokenizer.mask_token\n",
        "\n",
        "        masked_sequence = \"\".join(tokens)\n",
        "        inputs = tokenizer(masked_sequence, return_tensors=\"pt\", truncation=True, max_length=1024, padding='max_length')\n",
        "\n",
        "        # Compute the MLM loss\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Return the average loss\n",
        "    return total_loss / iterations\n",
        "\n",
        "# Compute MLM loss for each potential binder\n",
        "mlm_losses = {}\n",
        "for binder in potential_binders:\n",
        "    loss = compute_mlm_loss(protein_of_interest, binder)\n",
        "    mlm_losses[binder] = loss\n",
        "\n",
        "# Rank binders based on MLM loss\n",
        "ranked_binders = sorted(mlm_losses, key=mlm_losses.get)\n",
        "\n",
        "print(\"Ranking of Potential Binders:\")\n",
        "for idx, binder in enumerate(ranked_binders, 1):\n",
        "    print(f\"{idx}. {binder} - MLM Loss: {mlm_losses[binder]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y73kCRKGSJrL",
        "outputId": "6d079320-ae79-47a5-9eae-d0264ca62913"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking of Potential Binders:\n",
            "1. SEEEEERERALKEIIEETRRELKAAKAKHGKVVVVLIMASSTLEPEFILELSKALIKEMKSLFPNVVLIIVVVGLAPASLLARIRDVSLELAKYAKSLGIKVIVIVGNENEAVFVPAFEALGVEVIVDRTIIEIAAEELGLSEEEVLARFAAAAELLDELFAADPSLRERYARLDVAGATELLLERLRELFGAKVERHERLITVEVERVLTPDERRRVTAILLTPEAAREVVERLVDLVVDLILEKIAEGHNVLVLVFTPTIALAREVAALFEERRPLLEEAGAAVIIRLVARDPDTFLI - MLM Loss: 6.843159993489583\n",
            "2. LEEKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYDLSFLKTIQEVAGYVLIALNTVERIPLENLQIIRGNMYYENSYALAVLSNYDANKTGLKELPMRNLQEILHGAVRFSNNPALCNVESIQWRDIVSSDFLSNMSMDFQNHLGSCQKCDPSCPNGSCW - MLM Loss: 8.65021832784017\n",
            "3. GRPPKNVKVSGVDGNSATISFDLADENDKYILIMIGPANDPNSWTSWWLPHETSYLSISNLPPGAEYQVTFMMVRPGKMSPPITQDFKC - MLM Loss: 10.40660031636556\n",
            "4. KPQRKTYYGNMKGREDYEPEQSKEVYAKKFASKTEEELEEVIKEEKAEIEKKKKQLEEDIKAGKVTEYNPKVKVITPVYPSEYKEVEE - MLM Loss: 10.430347124735514\n"
          ]
        }
      ]
    }
  ]
}